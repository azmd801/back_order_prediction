{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNmRt+5whrcPcpZH9r+BI1n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azmd801/back_order_prediction/blob/main/notebook/Data_transformation_%26_Model_Selection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Approcah for data transformation"
      ],
      "metadata": {
        "id": "gcyP4TOolYt7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are lot of options availble for preprocessing data all these options are hyperparameters and can be cross validated to find the best preprocessing pipelin\n",
        "\n",
        "### Fixed steps which will done initially before any data transformation\n",
        "* dropping unncessary features  `'sku'`\n",
        "\n",
        "### Diffrent pre-processing optiona availbale\n",
        "\n",
        "1. use all the data as it is without any modification\n",
        "2. Use winsorization to handle outlier\n",
        "3. removing extreme sparse feature  `'in_transit_qty'`, `'local_bo_qty'`, `'pieces_past_due'`\n",
        "4. Removing extreme imbalance features `'potential_issue'`, `'oe_constraint'`, and `'rev_stop`\n",
        "5. Removing extreme sparse features\n",
        "\n",
        "### Options for Handling categorical feature\n",
        "1. Use one hot encode by removing first dummy variable\n",
        "2.Keeping all the variabbles\n",
        "\n",
        "### Options for imputation\n",
        "\n",
        " * `lead_time` has 100894 missing records (6%) which is large so these records cant be dropped\n",
        " * For rest of the features there is only one missing records so these will be dropped\n",
        " * For handiling the missing value of lead_time knn_imputer and median imputaion will be used\n",
        " * add_indicator will be explored\n",
        "\n",
        "### Refrences\n",
        "\n",
        " * https://chinmaydalvi.medium.com/backorder-prediction-using-machine-learning-cbe2a7d2cfa4\n"
      ],
      "metadata": {
        "id": "u82caR-iagM7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing required libraries"
      ],
      "metadata": {
        "id": "lZAEXyA0va3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from typing import List\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from imblearn.pipeline import Pipeline as Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, StandardScaler,LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.impute import KNNImputer\n",
        "from scipy.stats.mstats import winsorize\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# intalling required libraries\n",
        "!pip install --quiet  unrar\n",
        "!pip install --quiet  patool\n",
        "import os\n",
        "import patoolib\n",
        "\n",
        "!pip install --quiet optuna\n",
        "import optuna\n",
        "\n",
        "optuna.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "rpq1B819aetL",
        "outputId": "cb904153-497e-4839-855b-86ba13cb1a14"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.5/77.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m390.6/390.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.2.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom functions"
      ],
      "metadata": {
        "id": "5WrCbTEdvqoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Some functions for creating feature transformation pipeline\n",
        "\n",
        "## function to drop columns\n",
        "\n",
        "def drop_col(df: pd.DataFrame, columns: List[str], drop: bool =True) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Drop columns from a DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    - df: Input DataFrame.\n",
        "    - columns: List of column names to drop.\n",
        "\n",
        "    Returns:\n",
        "    - DataFrame: DataFrame with the specified columns dropped.\n",
        "    \"\"\"\n",
        "    # if 'sku' in df.columns:\n",
        "    #   df.drop('sku',inplace=True,axis=1)\n",
        "\n",
        "    if not drop:\n",
        "      return df.drop('sku',axis=1)\n",
        "\n",
        "    return df.drop('sku',axis=1).drop(columns, axis=1)\n",
        "\n",
        "from sklearn.base import TransformerMixin\n",
        "\n",
        "## custom class with fit and tranform to perform winsorization\n",
        "\n",
        "class Winsorizer(TransformerMixin):\n",
        "    def __init__(self, change=True):\n",
        "        \"\"\"\n",
        "        Initialize the Winsorizer transformer.\n",
        "\n",
        "        Parameters:\n",
        "        - lower_quantile (float): Lower quantile for winsorization (default: 0.05).\n",
        "        - upper_quantile (float): Upper quantile for winsorization (default: 0.95).\n",
        "        \"\"\"\n",
        "        self.change = change\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Fit the Winsorizer transformer.\n",
        "\n",
        "        Parameters:\n",
        "        - X (array-like): Input data.\n",
        "        - y: Ignored.\n",
        "\n",
        "        Returns:\n",
        "        - self: Returns the instance of the transformer.\n",
        "        \"\"\"\n",
        "        # Calculate the percentiles\n",
        "        p0 = np.nanpercentile(X, 0)\n",
        "        p100 = np.nanpercentile(X, 100)\n",
        "\n",
        "        # Calculate the lower and upper IQR\n",
        "        Q1 = np.nanpercentile(X, 25)\n",
        "        Q3 = np.nanpercentile(X, 75)\n",
        "        IQR = Q3 - Q1\n",
        "\n",
        "        # Calculate the lower and upper bounds\n",
        "        self.lower_bound = max(Q1 - (1.5 * IQR),p0)\n",
        "        self.upper_bound = min(Q3 + (1.5 * IQR),p100)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Transform the input data using winsorization.\n",
        "\n",
        "        Parameters:\n",
        "        - X (array-like): Input data to be transformed.\n",
        "\n",
        "        Returns:\n",
        "        - X_transformed (array-like): Transformed data after winsorization.\n",
        "        \"\"\"\n",
        "\n",
        "        if not self.change:\n",
        "          return X\n",
        "\n",
        "        X_clipped = np.clip(X, self.lower_bound, self.upper_bound)\n",
        "        return X_clipped\n",
        "\n",
        "    def get_feature_names_out(self, input_features):\n",
        "        \"\"\"\n",
        "        Get the feature names after transformation.\n",
        "\n",
        "        Parameters:\n",
        "        - input_features (array-like): Input feature names.\n",
        "\n",
        "        Returns:\n",
        "        - output_features (array-like): Transformed feature names.\n",
        "        \"\"\"\n",
        "        return input_features"
      ],
      "metadata": {
        "id": "g7WvlkkMt6n8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-rKcwLRO3PBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Ingestion"
      ],
      "metadata": {
        "id": "2Qanj-wuZAYP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "Ji7vGgK7WtlH",
        "outputId": "b34eebe7-2f4a-4465-a58e-457b1db57902"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-03 12:42:27--  https://github.com/rodrigosantis1/backorder_prediction/raw/master/dataset.rar\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/rodrigosantis1/backorder_prediction/master/dataset.rar [following]\n",
            "--2023-08-03 12:42:27--  https://raw.githubusercontent.com/rodrigosantis1/backorder_prediction/master/dataset.rar\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 24741696 (24M) [application/octet-stream]\n",
            "Saving to: ‘dataset/dataset.rar’\n",
            "\n",
            "dataset/dataset.rar 100%[===================>]  23.59M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-08-03 12:42:28 (191 MB/s) - ‘dataset/dataset.rar’ saved [24741696/24741696]\n",
            "\n",
            "patool: Extracting dataset/dataset.rar ...\n",
            "patool: running /usr/bin/unrar x -- /content/dataset/dataset.rar\n",
            "patool:     with cwd='dataset'\n",
            "patool: ... dataset/dataset.rar extracted to `dataset'.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'dataset'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "\n",
        "# Create a directory to store the dataset\n",
        "if not os.path.exists(\"dataset\"):\n",
        "    os.makedirs(\"dataset\")\n",
        "\n",
        "# Download the RAR file from GitHub\n",
        "!wget -O dataset/dataset.rar https://github.com/rodrigosantis1/backorder_prediction/raw/master/dataset.rar\n",
        "\n",
        "# Extract the RAR file\n",
        "patoolib.extract_archive(\"dataset/dataset.rar\", outdir=\"dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reading th data\n",
        "raw_data = pd.read_csv(r\"/content/dataset/Kaggle_Training_Dataset_v2.csv\")\n",
        "raw_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_JU4LKkpFSX",
        "outputId": "5efdc060-c205-4aff-8766-131fc41ea708"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1687861, 23)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "2efQtjdnZZkO",
        "outputId": "d860d1d8-fd48-4496-8e97-f558f9939eca"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                    sku  national_inv  lead_time  in_transit_qty  \\\n",
              "0               1026827           0.0        NaN             0.0   \n",
              "1               1043384           2.0        9.0             0.0   \n",
              "2               1043696           2.0        NaN             0.0   \n",
              "3               1043852           7.0        8.0             0.0   \n",
              "4               1044048           8.0        NaN             0.0   \n",
              "...                 ...           ...        ...             ...   \n",
              "1687856         1373987          -1.0        NaN             0.0   \n",
              "1687857         1524346          -1.0        9.0             0.0   \n",
              "1687858         1439563          62.0        9.0            16.0   \n",
              "1687859         1502009          19.0        4.0             0.0   \n",
              "1687860  (1687860 rows)           NaN        NaN             NaN   \n",
              "\n",
              "         forecast_3_month  forecast_6_month  forecast_9_month  sales_1_month  \\\n",
              "0                     0.0               0.0               0.0            0.0   \n",
              "1                     0.0               0.0               0.0            0.0   \n",
              "2                     0.0               0.0               0.0            0.0   \n",
              "3                     0.0               0.0               0.0            0.0   \n",
              "4                     0.0               0.0               0.0            0.0   \n",
              "...                   ...               ...               ...            ...   \n",
              "1687856               5.0               7.0               9.0            1.0   \n",
              "1687857               7.0               9.0              11.0            0.0   \n",
              "1687858              39.0              87.0             126.0           35.0   \n",
              "1687859               0.0               0.0               0.0            2.0   \n",
              "1687860               NaN               NaN               NaN            NaN   \n",
              "\n",
              "         sales_3_month  sales_6_month  ...  pieces_past_due  perf_6_month_avg  \\\n",
              "0                  0.0            0.0  ...              0.0            -99.00   \n",
              "1                  0.0            0.0  ...              0.0              0.99   \n",
              "2                  0.0            0.0  ...              0.0            -99.00   \n",
              "3                  0.0            0.0  ...              0.0              0.10   \n",
              "4                  0.0            0.0  ...              0.0            -99.00   \n",
              "...                ...            ...  ...              ...               ...   \n",
              "1687856            3.0            3.0  ...              0.0            -99.00   \n",
              "1687857            8.0           11.0  ...              0.0              0.86   \n",
              "1687858           63.0          153.0  ...              0.0              0.86   \n",
              "1687859            7.0           12.0  ...              0.0              0.73   \n",
              "1687860            NaN            NaN  ...              NaN               NaN   \n",
              "\n",
              "        perf_12_month_avg  local_bo_qty  deck_risk  oe_constraint  ppap_risk  \\\n",
              "0                  -99.00           0.0         No             No         No   \n",
              "1                    0.99           0.0         No             No         No   \n",
              "2                  -99.00           0.0        Yes             No         No   \n",
              "3                    0.13           0.0         No             No         No   \n",
              "4                  -99.00           0.0        Yes             No         No   \n",
              "...                   ...           ...        ...            ...        ...   \n",
              "1687856            -99.00           1.0         No             No         No   \n",
              "1687857              0.84           1.0        Yes             No         No   \n",
              "1687858              0.84           6.0         No             No         No   \n",
              "1687859              0.78           1.0         No             No         No   \n",
              "1687860               NaN           NaN        NaN            NaN        NaN   \n",
              "\n",
              "        stop_auto_buy rev_stop went_on_backorder  \n",
              "0                 Yes       No                No  \n",
              "1                 Yes       No                No  \n",
              "2                 Yes       No                No  \n",
              "3                 Yes       No                No  \n",
              "4                 Yes       No                No  \n",
              "...               ...      ...               ...  \n",
              "1687856           Yes       No                No  \n",
              "1687857            No       No               Yes  \n",
              "1687858           Yes       No                No  \n",
              "1687859           Yes       No                No  \n",
              "1687860           NaN      NaN               NaN  \n",
              "\n",
              "[1687861 rows x 23 columns]"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-1f9a2684-e5bc-49a9-ba1b-5f9f3c23a836\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sku</th>\n",
              "      <th>national_inv</th>\n",
              "      <th>lead_time</th>\n",
              "      <th>in_transit_qty</th>\n",
              "      <th>forecast_3_month</th>\n",
              "      <th>forecast_6_month</th>\n",
              "      <th>forecast_9_month</th>\n",
              "      <th>sales_1_month</th>\n",
              "      <th>sales_3_month</th>\n",
              "      <th>sales_6_month</th>\n",
              "      <th>...</th>\n",
              "      <th>pieces_past_due</th>\n",
              "      <th>perf_6_month_avg</th>\n",
              "      <th>perf_12_month_avg</th>\n",
              "      <th>local_bo_qty</th>\n",
              "      <th>deck_risk</th>\n",
              "      <th>oe_constraint</th>\n",
              "      <th>ppap_risk</th>\n",
              "      <th>stop_auto_buy</th>\n",
              "      <th>rev_stop</th>\n",
              "      <th>went_on_backorder</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1026827</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-99.00</td>\n",
              "      <td>-99.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1043384</td>\n",
              "      <td>2.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.0</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1043696</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-99.00</td>\n",
              "      <td>-99.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1043852</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.0</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1044048</td>\n",
              "      <td>8.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-99.00</td>\n",
              "      <td>-99.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1687856</th>\n",
              "      <td>1373987</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-99.00</td>\n",
              "      <td>-99.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1687857</th>\n",
              "      <td>1524346</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.84</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1687858</th>\n",
              "      <td>1439563</td>\n",
              "      <td>62.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>126.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>63.0</td>\n",
              "      <td>153.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.84</td>\n",
              "      <td>6.0</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1687859</th>\n",
              "      <td>1502009</td>\n",
              "      <td>19.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.78</td>\n",
              "      <td>1.0</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1687860</th>\n",
              "      <td>(1687860 rows)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1687861 rows × 23 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1f9a2684-e5bc-49a9-ba1b-5f9f3c23a836')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-51a70bb8-5baa-4a26-b022-d1e2693e17ce\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-51a70bb8-5baa-4a26-b022-d1e2693e17ce')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-51a70bb8-5baa-4a26-b022-d1e2693e17ce button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1f9a2684-e5bc-49a9-ba1b-5f9f3c23a836 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1f9a2684-e5bc-49a9-ba1b-5f9f3c23a836');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data cleaning"
      ],
      "metadata": {
        "id": "BTKRddsdAkr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dropping the na record for all the features except for feature 'lead_time'\n",
        "\n",
        "print(f\"Shape of data before dropping na {raw_data.shape}\\n\")\n",
        "raw_data.dropna(subset=raw_data.columns.drop('lead_time'),inplace=True)\n",
        "print(f\"Shape of data after dropping na {raw_data.shape}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_zy3AIJxY-8",
        "outputId": "70d709fb-b240-4ae9-ccfa-171bed21c5d9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of data before dropping na (1687861, 23)\n",
            "\n",
            "Shape of data after dropping na (1687860, 23)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Expermentin with different ML training pipeline using optuna   "
      ],
      "metadata": {
        "id": "HY5O4EyvAttu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing required Variables for optimizing the objective function"
      ],
      "metadata": {
        "id": "g727vQzMp1pz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFalZNzmu_sD",
        "outputId": "ccbdd5eb-98b3-4778-d804-17c3fa394576"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sku                   object\n",
              "national_inv         float64\n",
              "lead_time            float64\n",
              "in_transit_qty       float64\n",
              "forecast_3_month     float64\n",
              "forecast_6_month     float64\n",
              "forecast_9_month     float64\n",
              "sales_1_month        float64\n",
              "sales_3_month        float64\n",
              "sales_6_month        float64\n",
              "sales_9_month        float64\n",
              "min_bank             float64\n",
              "potential_issue       object\n",
              "pieces_past_due      float64\n",
              "perf_6_month_avg     float64\n",
              "perf_12_month_avg    float64\n",
              "local_bo_qty         float64\n",
              "deck_risk             object\n",
              "oe_constraint         object\n",
              "ppap_risk             object\n",
              "stop_auto_buy         object\n",
              "rev_stop              object\n",
              "went_on_backorder     object\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take 10000 random samples from the dataset while stratifying based on a specific column (e.g., 'label')\n",
        "# sample_size = 100000\n",
        "# samples = raw_data.sample(n=sample_size, replace=False)\n",
        "# samples.head()\n",
        "\n",
        "# # spliting X_train and y_train\n",
        "# y_train = LabelEncoder().fit_transform(samples['went_on_backorder'])\n",
        "# X_train = samples.drop('went_on_backorder',axis=1)\n",
        "\n",
        "# print(X_train.shape)\n",
        "# print(y_train.shape)"
      ],
      "metadata": {
        "id": "FtO8gr1EfMaz",
        "outputId": "e479b03a-88d3-4c67-9754-666f8504d801",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100000, 22)\n",
            "(100000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# spliting X_train and y_train\n",
        "y_train = LabelEncoder().fit_transform(raw_data['went_on_backorder'])\n",
        "X_train = raw_data.drop('went_on_backorder',axis=1)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "id": "sTP5cmhm33XE",
        "outputId": "87e8abac-09bf-4dde-e25c-08c1220de7a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1687860, 22)\n",
            "(1687860,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "id": "dee0co8ZpyXE",
        "outputId": "3e58d250-a4ad-4756-c1fb-67488810fba7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 1, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# segregating numeric and categorical fetaure\n",
        "numeric_cols = X_train.columns[X_train.dtypes != 'object']\n",
        "print(f'Numeric columns: {numeric_cols}\\n')\n",
        "\n",
        "cat_cols = X_train.columns[X_train.dtypes == 'object'].drop('sku')\n",
        "print(f'Categorical columns: {cat_cols}\\n')\n",
        "\n",
        "print(f'Target columns : went_on_backorder')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMRkDXFylhJ4",
        "outputId": "0907576c-51ff-4b22-cb4d-a769c8fb1825"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numeric columns: Index(['national_inv', 'lead_time', 'in_transit_qty', 'forecast_3_month',\n",
            "       'forecast_6_month', 'forecast_9_month', 'sales_1_month',\n",
            "       'sales_3_month', 'sales_6_month', 'sales_9_month', 'min_bank',\n",
            "       'pieces_past_due', 'perf_6_month_avg', 'perf_12_month_avg',\n",
            "       'local_bo_qty'],\n",
            "      dtype='object')\n",
            "\n",
            "Categorical columns: Index(['potential_issue', 'deck_risk', 'oe_constraint', 'ppap_risk',\n",
            "       'stop_auto_buy', 'rev_stop'],\n",
            "      dtype='object')\n",
            "\n",
            "Target columns : went_on_backorder\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "useless_cols = ['in_transit_qty', 'local_bo_qty', 'pieces_past_due','potential_issue', 'oe_constraint','rev_stop']"
      ],
      "metadata": {
        "id": "HnWztJGb85Mb"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the proportion of zeros in each column\n",
        "zero_proportion = (X_train == 0).sum() / len(X_train)\n",
        "\n",
        "# Filter features with more than 30 percent zeros\n",
        "threshold = 0.3\n",
        "filtered_data = X_train.drop(columns=zero_proportion[zero_proportion < threshold].index)\n",
        "\n",
        "# Get the feature names of the remaining columns\n",
        "sparse_cols = filtered_data.columns.tolist()\n",
        "\n",
        "sparse_cols"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFCDVenW9E4D",
        "outputId": "64e07437-76a7-4ff8-91de-706bfae56c0e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['in_transit_qty',\n",
              " 'forecast_3_month',\n",
              " 'forecast_6_month',\n",
              " 'forecast_9_month',\n",
              " 'sales_1_month',\n",
              " 'sales_3_month',\n",
              " 'sales_6_month',\n",
              " 'sales_9_month',\n",
              " 'min_bank',\n",
              " 'pieces_past_due',\n",
              " 'local_bo_qty']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPipeline:\n",
        "    def __init__(self, imputer_type, drop_and_winsorize, classifier, columns_to_drop,\n",
        "                 add_indicator=False, keep_first=False):\n",
        "\n",
        "        self.imputer_type = imputer_type\n",
        "        self.drop_and_winsorize = drop_and_winsorize\n",
        "        self.classifier = classifier\n",
        "        self.columns_to_drop = columns_to_drop\n",
        "        self.useless_cols = useless_cols\n",
        "        self.sparse_cols = sparse_cols\n",
        "        self.numeric_cols = numeric_cols\n",
        "        self.cat_cols = cat_cols\n",
        "        self.add_indicator = add_indicator\n",
        "        self.keep_first = keep_first\n",
        "\n",
        "        if self.drop_and_winsorize:\n",
        "            self.drop_cols = self.useless_cols if self.columns_to_drop == 'useless' else self.sparse_cols\n",
        "        else:\n",
        "            self.drop_cols = []\n",
        "\n",
        "        self.num_cols_used = [col for col in self.numeric_cols if col not in self.drop_cols]\n",
        "        self.cat_cols_used = [col for col in self.cat_cols if col not in self.drop_cols]\n",
        "\n",
        "    def _create_classifier(self):\n",
        "        if self.classifier == 'RandomForest':\n",
        "            clf = RandomForestClassifier()\n",
        "\n",
        "        elif self.classifier == 'SVC':\n",
        "            clf = SVC()\n",
        "\n",
        "        elif self.classifier == 'LogisticRegression':\n",
        "            clf = LogisticRegression(solver='liblinear')\n",
        "\n",
        "        elif self.classifier == 'XGBoost':\n",
        "            clf = XGBClassifier()\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid classifier: {self.classifier}\")\n",
        "\n",
        "        return clf\n",
        "\n",
        "    def _drop_col(self, X):\n",
        "        if not self.drop_and_winsorize:\n",
        "          # Always drop 'sku' column and additional columns specified in drop_cols\n",
        "            X.drop('sku',axis=1)\n",
        "\n",
        "        columns_to_drop = ['sku'] + self.drop_cols\n",
        "        return X.drop(columns_to_drop, axis=1)\n",
        "\n",
        "    def create_pipeline(self):\n",
        "        # Conditional creation of imputer based on 'imputer_type'\n",
        "        if self.imputer_type == 'knn':\n",
        "            imp = KNNImputer(weights='distance', add_indicator=self.add_indicator)\n",
        "        else:\n",
        "            imp = SimpleImputer(strategy='median', add_indicator=self.add_indicator)\n",
        "\n",
        "        # Conditional initialization of cat_encoder based on 'keep_first'\n",
        "        cat_encoder = OneHotEncoder(drop='first') if self.keep_first else OneHotEncoder()\n",
        "\n",
        "        # Constructing the numerical pipeline\n",
        "        num_pipeline = Pipeline([\n",
        "            ('scaler', StandardScaler()),\n",
        "            ('imputer', imp),\n",
        "            ('outlier_clipping', Winsorizer(change=self.drop_and_winsorize)),\n",
        "        ])\n",
        "\n",
        "        # Constructing the categorical pipeline\n",
        "        cat_pipeline = Pipeline([\n",
        "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "            ('onehotencoder', cat_encoder),\n",
        "        ])\n",
        "\n",
        "        # Constructing the training pipeline\n",
        "        clf = self._create_classifier()\n",
        "        training_pipeline = Pipeline([\n",
        "            ('Drop_Columns', FunctionTransformer(self._drop_col)),\n",
        "            ('Balancing', RandomOverSampler()),\n",
        "            ('Feature_transform', ColumnTransformer([\n",
        "                ('num_pipeline', num_pipeline, self.num_cols_used),\n",
        "                ('cat_pipeline', cat_pipeline, self.cat_cols_used),\n",
        "            ])),\n",
        "            ('model', clf)\n",
        "        ])\n",
        "\n",
        "        return training_pipeline"
      ],
      "metadata": {
        "id": "s8ycsqRKPGhW"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into train and validation sets (80% train, 20% validation)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.4, random_state=42)\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of X_val:\", X_val.shape)\n",
        "print(\"Shape of y_val:\", y_val.shape)"
      ],
      "metadata": {
        "id": "j01ypXCTixdP",
        "outputId": "cf498b51-afb3-4e84-9621-67a132bede24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train: (1012716, 22)\n",
            "Shape of y_train: (1012716,)\n",
            "Shape of X_val: (675144, 22)\n",
            "Shape of y_val: (675144,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizing objective fucntion to get best ML pipeline"
      ],
      "metadata": {
        "id": "sX5dnFPyqmQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import balanced_accuracy_score"
      ],
      "metadata": {
        "id": "dIntdpEM1nVv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining Objective function which will be optimized\n",
        "\n",
        "def objective(trial):\n",
        "\n",
        "    # Sampling the classifiers\n",
        "    classifier = trial.suggest_categorical('classifier', ['RandomForest', 'SVC', 'LogisticRegression', 'XGBoost'])\n",
        "\n",
        "    # experimenting with dropping unnecessary columns and outlier handling\n",
        "    drop_and_winsorize = trial.suggest_int('drop_and_winsorize',0,1)\n",
        "\n",
        "    # experimenting with columns to drop\n",
        "    columns_to_drop = trial.suggest_categorical('columns_to_drop', ['useless', 'sparse'])\n",
        "\n",
        "    # experimenting with imputers\n",
        "    add_indicator = trial.suggest_int('add_indicator',0,1)\n",
        "    imputer = trial.suggest_categorical('imputer_type',['simple','knn'])\n",
        "\n",
        "    # experimenting with one hot encoding\n",
        "    keep_first = trial.suggest_int('keep_first',0,1)\n",
        "\n",
        "    pipeline_params = {\n",
        "    'imputer_type': imputer,\n",
        "    'drop_and_winsorize': drop_and_winsorize,\n",
        "    'classifier': classifier,\n",
        "    'columns_to_drop': columns_to_drop,\n",
        "    'add_indicator': add_indicator,\n",
        "    'keep_first': keep_first\n",
        "    }\n",
        "    pipeline = MLPipeline(**pipeline_params)\n",
        "\n",
        "   # constructing training pipeline\n",
        "    training_pipeline = pipeline.create_pipeline()\n",
        "\n",
        "    training_pipeline.fit(X_train,y_train)\n",
        "\n",
        "    scores = balanced_accuracy_score(y_val, training_pipeline.predict(X_val))\n",
        "    # scorer = make_scorer(balanced_accuracy_score)\n",
        "\n",
        "    # scores = cross_val_score(training_pipeline, X_train, y_train, n_jobs=-1, cv=5, scoring=scorer)\n",
        "    # report_cross_validation_scores(trial, scores)\n",
        "\n",
        "    # Returning the cross-validated mean score\n",
        "    return scores.mean()\n"
      ],
      "metadata": {
        "id": "4kx0gLGuf_BA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study = optuna.create_study(direction=\"maximize\")\n",
        "# terminator = TerminatorCallback()\n",
        "study.optimize(objective, n_trials=100, n_jobs=-1, )#, #callbacks=[terminator])\n",
        "trial = study.best_trial\n",
        "\n",
        "print('f1_score: {}'.format(trial.value))\n",
        "print(\"Best hyperparameters: {}\".format(trial.params))"
      ],
      "metadata": {
        "id": "OpLhPFxghwvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter tuning of the  models using optuna"
      ],
      "metadata": {
        "id": "Z078dr2a9Nn7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Due to lack of resuurces we find the best pilpiline using 100000 snamples (10% of whole data)\n",
        "\n",
        "* Optuna giving a pipeline having Logistincregressin as eatimator due to such s sample size we dont have confidence in the model\n",
        "\n",
        "* We will perform cross validation to find the best hypermaters of multiple models of using transformation obtained from best piple using whole data"
      ],
      "metadata": {
        "id": "qzUCwacd3SHD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finding best model"
      ],
      "metadata": {
        "id": "rYZKgX_f4Zsf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Best pipeline parameters obtained from cross validation using optuna\n",
        "best_pipeline_param = {\n",
        "    'classifier': 'LogisticRegression',\n",
        "    'drop_and_winsorize': 1,\n",
        "    'columns_to_drop': 'useless',\n",
        "    'add_indicator': 1,\n",
        "    'imputer_type': 'simple',\n",
        "    'keep_first': 1\n",
        "}\n",
        "\n",
        "# Creating the best MLPipeline with the best hyperparameters\n",
        "best_ml_pipeline = MLPipeline(**best_pipeline_param).create_pipeline()\n",
        "\n",
        "# Lists to store train and validation scores for each classifier\n",
        "train_score_list = []\n",
        "val_score_list = []\n",
        "\n",
        "# List of classifiers to evaluate\n",
        "clf_list = [XGBClassifier(), RandomForestClassifier(), LogisticRegression(), SVC()]\n",
        "\n",
        "for clf in clf_list:\n",
        "    # Updating the classifier in the pipeline\n",
        "    best_ml_pipeline.steps[-1] = ('model', clf)\n",
        "\n",
        "    # Fitting the pipeline to the training data\n",
        "    best_ml_pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluating the train score and storing it\n",
        "    train_score = balanced_accuracy_score(y_train, best_ml_pipeline.predict(X_train))\n",
        "    train_score_list.append(train_score)\n",
        "\n",
        "    # Evaluating the validation score and storing it\n",
        "    val_score = balanced_accuracy_score(y_val, best_ml_pipeline.predict(X_val))\n",
        "    val_score_list.append(val_score)\n",
        "\n",
        "    # Printing the train and validation scores for the current classifier\n",
        "    print(f\"\"\"\n",
        "    For model {clf}:\n",
        "        Train score = {train_score}\n",
        "        Validation score = {val_score}\n",
        "    \"\"\")"
      ],
      "metadata": {
        "id": "efr2hbGUqo3_",
        "outputId": "a23b99ac-dcb9-4b2b-e3c0-d91793f13c68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For model XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=None, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
            "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
            "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
            "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
            "              predictor=None, random_state=None, ...):\n",
            "\n",
            "        Train score = 0.9352394432543951\n",
            "        Validation score = 0.8797050483251394\n",
            "\n",
            "        \n",
            "        \n",
            "For model RandomForestClassifier():\n",
            "\n",
            "        Train score = 0.9845333385365995\n",
            "        Validation score = 0.6566953094014734\n",
            "\n",
            "        \n",
            "        \n",
            "For model LogisticRegression():\n",
            "\n",
            "        Train score = 0.7747810157953924\n",
            "        Validation score = 0.7817776664563587\n",
            "\n",
            "        \n",
            "        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* XGboost is gving the best performance using the best pipline obtained\n",
        "* Now we will find best hyper parameters XGboost to further improve the performance"
      ],
      "metadata": {
        "id": "SdDhDNOXUenz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter tuning of the best model"
      ],
      "metadata": {
        "id": "He0sUuTNVA3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating objective function to find the besst hyperparameter\n",
        "\n",
        "def objective_hp_tuning(trial):\n",
        "\n",
        "  # Best pipeline parameters obtained from cross validation using optuna\n",
        "  best_pipeline_param = {\n",
        "      'classifier': 'XGBoost',\n",
        "      'drop_and_winsorize': 1,\n",
        "      'columns_to_drop': 'useless',\n",
        "      'add_indicator': 1,\n",
        "      'imputer_type': 'simple',\n",
        "      'keep_first': 1\n",
        "  }\n",
        "\n",
        "  # Creating the best MLPipeline\n",
        "  best_ml_pipeline = MLPipeline(**best_pipeline_param).create_pipeline()\n",
        "\n",
        "  # Sampling hyperparameters for XGBoostClassifier\n",
        "  n_estimators = trial.suggest_int('xgb_n_estimators', 50, 300, step=50)\n",
        "  max_depth = trial.suggest_int('xgb_max_depth', 3, 10)\n",
        "  learning_rate = trial.suggest_loguniform('xgb_learning_rate', 0.01, 0.1)\n",
        "  subsample = trial.suggest_uniform('xgb_subsample', 0.6, 0.9)\n",
        "\n",
        "  # putting the hyperparameter in kwarg args\n",
        "  hyperparams = {\n",
        "    'model__n_estimators': n_estimators,\n",
        "    'model__max_depth': max_depth,\n",
        "    'model__learning_rate': learning_rate,\n",
        "    'model__subsample': subsample\n",
        "  }\n",
        "\n",
        "  # passing the hyperparameters to the estimator\n",
        "  best_ml_pipeline.set_params(**hyperparams)\n",
        "\n",
        "  # Fitting the pipeline to the training data\n",
        "  best_ml_pipeline.fit(X_train, y_train)\n",
        "\n",
        "  # Evaluating the validation score\n",
        "  val_score = balanced_accuracy_score(y_val, best_ml_pipeline.predict(X_val))\n",
        "\n",
        "  return val_score\n"
      ],
      "metadata": {
        "id": "4C0jsCjGU9Rc"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an Optuna study object for hyperparameter tuning with \"maximize\" direction to maximize the objective function\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "\n",
        "# Perform hyperparameter tuning using the objective_hp_tuning function as the objective function\n",
        "# n_trials: Number of trials (iterations) for hyperparameter tuning\n",
        "# n_jobs: Number of parallel jobs to run in parallel. Use -1 to use all available CPU cores.\n",
        "study.optimize(objective_hp_tuning, n_trials=100, n_jobs=-1)\n",
        "\n",
        "# Get the best trial from the study (the trial with the highest value of the objective function)\n",
        "trial = study.best_trial\n",
        "\n",
        "# Print the balanced score obtained from the best trial\n",
        "print('balanced score: {}'.format(trial.value))\n",
        "\n",
        "# Print the best hyperparameters found from the hyperparameter tuning process\n",
        "print(\"Best hyperparameters: {}\".format(trial.params))"
      ],
      "metadata": {
        "id": "8cQ7mOdq2lnr",
        "outputId": "0872a1c3-45e2-4758-b98c-8c64f662db22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-08-03 12:47:22,925] A new study created in memory with name: no-name-5441d310-0f50-4984-84cc-b435d6c9c618\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wJD2PzUlaUCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Defining Objective function which will be optimized\n",
        "\n",
        "# def objective(trial):\n",
        "\n",
        "#     # Sampling the classifier type along with its parameter\n",
        "#     classifier = trial.suggest_categorical('classifier', ['RandomForest', 'SVC', 'LogisticRegression', 'XGBoost'])\n",
        "\n",
        "#     if classifier == 'RandomForest':\n",
        "#         # Sampling hyperparameters for RandomForestClassifier\n",
        "#         n_estimators = trial.suggest_int('n_estimators', 50, 300, step=50)\n",
        "#         max_depth = trial.suggest_int('max_depth', 5, 20)\n",
        "#         min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
        "#         min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
        "\n",
        "#         clf = RandomForestClassifier(\n",
        "#             n_estimators=n_estimators,\n",
        "#             max_depth=max_depth,\n",
        "#             min_samples_split=min_samples_split,\n",
        "#             min_samples_leaf=min_samples_leaf\n",
        "#         )\n",
        "\n",
        "#     elif classifier == 'SVC':\n",
        "#         # Sampling hyperparameters for SVC\n",
        "#         C = trial.suggest_loguniform('svc_C', 1e-3, 1e3)\n",
        "#         kernel = trial.suggest_categorical('svc_kernel', ['linear', 'poly', 'rbf', 'sigmoid'])\n",
        "#         gamma = trial.suggest_categorical('svc_gamma', ['scale', 'auto'])\n",
        "\n",
        "#         clf = SVC(C=C, kernel=kernel, gamma=gamma)\n",
        "\n",
        "#     elif classifier == 'LogisticRegression':\n",
        "#         # Sampling hyperparameters for LogisticRegression\n",
        "#         C = trial.suggest_loguniform('lr_C', 1e-3, 1e3)\n",
        "#         penalty = trial.suggest_categorical('lr_penalty', ['l1', 'l2'])\n",
        "\n",
        "#         clf = LogisticRegression(C=C, penalty=penalty, solver='liblinear')\n",
        "\n",
        "#     elif classifier == 'XGBoost':\n",
        "#         # Sampling hyperparameters for XGBoostClassifier\n",
        "#         n_estimators = trial.suggest_int('xgb_n_estimators', 50, 300, step=50)\n",
        "#         max_depth = trial.suggest_int('xgb_max_depth', 3, 10)\n",
        "#         learning_rate = trial.suggest_loguniform('xgb_learning_rate', 0.01, 0.1)\n",
        "#         subsample = trial.suggest_uniform('xgb_subsample', 0.6, 0.9)\n",
        "\n",
        "#         clf = XGBClassifier(\n",
        "#             n_estimators=n_estimators,\n",
        "#             max_depth=max_depth,\n",
        "#             learning_rate=learning_rate,\n",
        "#             subsample=subsample\n",
        "#         )\n",
        "\n",
        "#     # experimenting with dropping unnecessary columns and outlier handling\n",
        "#     drop_and_winsorize = trial.suggest_int('drop_and_winsorize',0,1)\n",
        "\n",
        "#     if drop_and_winsorize:\n",
        "#       # experimenting with columns to drop\n",
        "#       columns_to_drop = trial.suggest_categorical('columns_to_drop', ['useless', 'sparse'])\n",
        "#       drop_cols = useless_cols if columns_to_drop=='useless' else sparse_cols\n",
        "#       num_cols_used = [col for col in numeric_cols if col not in drop_cols]\n",
        "#       cat_cols_used = [col for col in cat_cols if col not in drop_cols]\n",
        "#     else:\n",
        "#       drop_cols, num_cols_used, cat_cols_used = [], [], []\n",
        "\n",
        "#     # experimenting with imputers\n",
        "#     add_indicator = trial.suggest_int('add_indicator',0,1)\n",
        "#     imputer = trial.suggest_categorical('imputer',['simple','knn'])\n",
        "\n",
        "#     if imputer == 'knn':\n",
        "#       imp = KNNImputer(weights='distance', add_indicator=add_indicator)\n",
        "#     else:\n",
        "#       imp = SimpleImputer(strategy='median', add_indicator=add_indicator)\n",
        "\n",
        "#     # experimenting with one hot encoding\n",
        "#     keep_first = trial.suggest_int('keep_first',0,1)\n",
        "#     cat_encoder = OneHotEncoder(drop='first') if keep_first else OneHotEncoder()\n",
        "\n",
        "#     # Constructing the numerical pipeline\n",
        "#     num_pipeline = Pipeline([\n",
        "#         ('scaler', StandardScaler()),\n",
        "#         ('imputer', imp),\n",
        "#         ('outlier_clipping', Winsorizer(change=drop_and_winsorize)),\n",
        "#     ])\n",
        "#     # constructing categorical pipeline\n",
        "#     cat_pipeline = Pipeline([\n",
        "#         ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "#         ('onehotencoder', cat_encoder),\n",
        "#     ])\n",
        "\n",
        "#     # constructing training pipeline\n",
        "#     training_pipeline = Pipeline([\n",
        "#         ('Drop_Columns', FunctionTransformer(drop_col, kw_args={'columns': drop_cols, 'drop': drop_and_winsorize})),\n",
        "#         ('Balancing', RandomOverSampler()),\n",
        "#         ('Feature_transform', ColumnTransformer([\n",
        "#             ('num_pipeline', num_pipeline, num_cols_used),\n",
        "#             ('cat_pipeline', cat_pipeline, cat_cols_used),\n",
        "#             ])),\n",
        "\n",
        "#     ('Model training', clf)\n",
        "#     ])\n",
        "\n",
        "#     scorer = make_scorer(f1_score)\n",
        "\n",
        "#     scores = cross_val_score(training_pipeline, X_train, y_train, n_jobs=-1, cv=5, scoring=scorer)\n",
        "#     # report_cross_validation_scores(trial, scores)\n",
        "\n",
        "#     # Returning the cross-validated mean score\n",
        "#     return scores.mean()\n"
      ],
      "metadata": {
        "id": "9V7ohvEVboTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# study = optuna.create_study(direction=\"maximize\")\n",
        "# # terminator = TerminatorCallback()\n",
        "# study.optimize(objective, n_trials=100, n_jobs=2, )#, #callbacks=[terminator])\n",
        "# trial = study.best_trial\n",
        "\n",
        "# print('f1_score: {}'.format(trial.value))\n",
        "# print(\"Best hyperparameters: {}\".format(trial.params))\n"
      ],
      "metadata": {
        "id": "-foBm_DLgsz_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}